\documentclass[a4paper,12pt]{ctexart}
\usepackage[utf8]{inputenc}
\usepackage{geometry}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{hyperref}
\usepackage{float}
\usepackage{listings}
\usepackage{xcolor}

\geometry{left=2.5cm,right=2.5cm,top=2.5cm,bottom=2.5cm}

% 代码高亮设置
\lstset{
    basicstyle=\small\ttfamily,
    columns=flexible,
    breaklines=true,
    backgroundcolor=\color{gray!10},
    frame=single,
    keywordstyle=\color{blue},
    commentstyle=\color{green!50!black},
    stringstyle=\color{red}
}

\title{\textbf{NLP新闻分类学习赛\\实验报告}}
\author{\textbf{队伍名：Attention请注意我} \\ 排名：rk3}
\date{\today}

\begin{document}

\maketitle

\section{团队成员与分工}

本项目采用“共同决策 + 分工落地”的协作机制：在方案设计阶段由两人共同调研、讨论并决策整体技术路线，
在实现阶段按职责拆分任务、各自负责落地。
为在兼顾效率与质量的前提下实现分工清晰、任务解耦，我们将工作划分为前后端两个角色，具体如下：

\begin{itemize}
    \item \textbf{郑亦航}（前端：数据与模型设计）
    \begin{itemize}
        \item 负责数据探索性分析（如文本长度分布、类别不平衡等），撰写分析报告，为后续建模与训练策略提供依据。
        \item 搭建端到端的数据预处理流水线，完成数据清洗、切分、标签重映射等脚本封装，确保全流程可复现。
        \item 设计并实现 HAT 模型基础架构，定义输入管线与关键模块接口，并完成模型骨架的单元测试。
    \end{itemize}

    \item \textbf{李梓煊}（后端：训练与部署优化）
    \begin{itemize}
        \item 实现 MLM 预训练流程，完成与数据管线、Tokenizer / ID 偏移的对接与适配，保证长文本场景下的稳定训练。
        \item 优化分类训练流程，覆盖梯度累积、混合精度训练、学习率调度等环节，在长文本场景下提升收敛速度与性能。
        \item 负责第二阶段微调，设计与调整损失函数（如不同 Loss 组合）。
        \item 实现推理部署：基于 K-fold，将各折最佳 HAT 模型用于长文档滑窗前向与窗口聚合，在此基础上进行多模型加权集成，输出最终预测结果。
    \end{itemize}
\end{itemize}


\section{问题定义}

本次比赛的任务是“新闻文本分类”。
\begin{itemize}
    \item \textbf{输入}：匿名处理后的字符 ID 序列（空格分隔），代表一篇新闻文本。数据经过脱敏处理，无法直接看到原始汉字。
    \item \textbf{输出}：将文本分类到 14 个类别之一（如财经、体育、娱乐等），对应的 Label ID 为 0-13。
    \item \textbf{挑战}：
    \begin{enumerate}
        \item \textbf{长文本结构复杂性}：许多样本长度远超 BERT 模型的 512 token 限制（部分达到 40,000+ token），且关键信息在文本中分布稀疏、跨段依赖长，分段式编码容易遗漏关键语义。
        
        \item \textbf{文本风格与领域差异}：数据来源多样，风格从新闻报道到口语评论均有，语气、表达与话题差异显著，使模型难以学习统一、稳健的特征表示。
        
        \item \textbf{类别极度不平衡}：训练集中类别数量分布长尾显著，最大与最小类别比例高达 42:1，模型在标准训练中容易偏向头部类别。
        
        \item \textbf{数据噪声显著}：存在空文本、重复文本、标签冲突、低质量内容等多种噪声，会影响训练稳定性并降低模型泛化能力。
        
        \item \textbf{计算资源与模型容量限制}：长文本 + 大规模模型导致显存压力大，限制可使用的 batch size、对抗训练策略及模型规模，使训练不够充分。
    \end{enumerate}
    
\end{itemize}

\section{数据分析与预处理}

\subsection{数据特征分析}
我们使用自行开发的 \texttt{analyze\_dataset.py} 脚本（调用内部 \texttt{TextLengthAnalyzer} 和 \texttt{ClassDistributionAnalyzer} 模块），对 200,000 条训练样本和 50,000 条测试样本进行了全量扫描。

\subsubsection{类别分布与不平衡}
分析结果显示，本任务为 14 类单标签分类任务，类别分布呈现极度长尾特征。
\begin{itemize}
    \item \textbf{极端不平衡}：最大类别（Label 0）拥有 38,918 条样本，而最小类别（Label 13）仅 908 条。最大最小比（Imbalance Ratio）高达 \textbf{43:1}。
    \item \textbf{策略启示}：如此悬殊的比例要求我们在 Loss 设计上必须引入类别平衡手段（如 Class Weighting 或 Focal Loss），否则模型将完全偏向头部类别。
\end{itemize}

\subsubsection{文本长度统计}
文本长度是选择模型架构的关键依据。统计结果表明数据包含大量超长文本：
\begin{itemize}
    \item \textbf{统计指标}：训练集文本平均长度（Mean）约为 907 tokens，中位数（Median）为 676。P90 分位点达到 1,796，P99 高达 4,228，最长文本甚至接近 58,000 tokens。
    \item \textbf{覆盖率分析}：
    \begin{itemize}
        \item 若截断到 BERT 标准的 512 长度，仅能完整覆盖约 37\% 的样本。
        \item 当上下文窗口扩展至 \textbf{4096} 时，样本完整覆盖率达到 \textbf{98.89\%}，Token 级别覆盖率达到 97.37\%。超过 10k tokens 的极端长文仅占 0.08\%。
    \end{itemize}
    \item \textbf{结论}：4096 是一个兼顾信息完整度与计算开销的理想上下文窗口长度，这直接确立了我们使用长文档模型（HAT）的必要性。
\end{itemize}

\subsubsection{词汇表 (Vocab)}
数据已经过匿名化处理，Token ID 原始分布在 0--7549 之间，总词表规模约为 6,977。
\begin{itemize}
    \item \textbf{特殊 Token 冲突}：分析发现 ID 0, 1, 2, 3, 4 在原始数据中均有出现。这意味着我们不能直接使用常规的 0(PAD), 1(UNK) 等定义，必须进行 ID 偏移映射。
    \item \textbf{OOV 情况}：测试集中未登录词（OOV）极低（仅占 token 总数 0.0004\%），表明词表覆盖极其全面。
\end{itemize}

\subsection{预处理流水线}
基于上述分析，我们设计了轻量级的预处理脚本 \texttt{run\_preprocessing.py}。值得强调的是，\textbf{文档分段（Segmentation）并未在此阶段进行}，预处理仅负责清洗和标准化，分段逻辑交由模型输入层动态处理以支持数据增强。

\subsubsection{Token ID 重映射}
为解决特殊 Token 冲突，我们将所有原始 Token ID 整体向右偏移 5 位：
\begin{equation}
    t' = t_{raw} + 5, \quad t_{raw} \in [0, 7549] \rightarrow t' \in [5, 7554]
\end{equation}
腾出的 [0, 4] 空间严格定义如下，最终 Vocab Size 为 7,555：
\begin{itemize}
    \item 0: \texttt{[PAD]}, 1: \texttt{[UNK]}, 2: \texttt{[CLS\_SEG]} (用于段级聚合), 3: \texttt{[SEP]}, 4: \texttt{[MASK]} (用于预训练)。
\end{itemize}

\subsubsection{数据清洗}
使用 \texttt{DataCleaner} 模块执行了严格过滤：
\begin{itemize}
    \item \textbf{去重}：移除了 153 条完全重复的文本（保留首条）。
    \item \textbf{冲突处理}：移除了 8 组“内容相同但标签不同”的冲突样本，避免噪声干扰。
    \item \textbf{异常值处理}：移除了长度为 0 的空文本。
\end{itemize}


\section{模型设计}

\subsection{设计目标与约束}
本赛题的核心在于在有限的算力和时间内，处理大规模长文本分类问题。我们的决策基于以下关键约束：
\begin{itemize}
    \item \textbf{数据约束}：20 万样本，约 2 亿 Token。由于数据为匿名 ID，无法利用现成预训练模型（如 BERT/LLaMA）的语义知识，必须\textbf{从零预训练}。
    \item \textbf{算力约束}：4 $\times$ H800 GPU，预训练时间窗口约为 24 小时。
    \item \textbf{长度约束}：需有效处理 4096 长度的上下文，且不能像截断 BERT 那样丢失尾部关键信息。
\end{itemize}

\subsection{架构选型对比}
基于上述约束，我们对比了四种主流技术路线，分析如下表所示：

\begin{table}[H]
    \centering
    \small
    \caption{长文档分类主流架构对比分析}
    \begin{tabular}{p{4cm} p{4cm} p{4cm} p{2.5cm}}
        \toprule
        \textbf{架构方案} & \textbf{优势} & \textbf{劣势}\\
        \midrule
        \textbf{大语言模型} \newline (LLaMA/Qwen 等) & 强大的生成能力与few-shot能力。 & 参数量大（7B+），需海量数据从零训练，2亿 token 极易导致过拟合；训练昂贵。 \\
        \midrule
        \textbf{截断版 BERT} \newline (Max Len 512) & 训练速度快，基建成熟。 & 丢失 50\%+ 信息（尤其是新闻/法律文本的尾部结论），性能上限低。\\
        \midrule
        \textbf{稀疏 Transformer} \newline (Longformer/BigBird) & 原生支持长上下文，复杂度 $O(N)$。 & 需要自定义 CUDA Kernel 或复杂实现；在 4k 长度下，显存优势不如分层模型明显。\\
        \midrule
        \textbf{层次化 Transformer} \newline (HAT/HDT) & \textbf{性价比最高}。结合了段内标准 Attention 的强特征提取与段间聚合的全局视野；显存占用低。 & 需要定制分层结构（Segment-wise + Cross-segment）。\\
        \bottomrule
    \end{tabular}
    \label{tab:model_comparison}
\end{table}

\textbf{决策结论}：考虑到 200k 样本足以训练一个 1 亿参数量级（BERT-Medium/Base）的模型，且 4096 长度正好适合分层处理（如 8 个 512 的片段），我们最终选择了 \textbf{HAT (Hierarchical Attention Transformer)} 架构。它在保持计算高效的同时，能完整保留长文档的结构化语义。

\subsection{总体架构概览}
模型整体采用“分段-交互-聚合”的设计思路，自底向上包含四个核心模块：
\begin{enumerate}
    \item \textbf{动态分段器 (Document Segmenter)}：负责将长序列切分为固定长度的 Segment。
    \item \textbf{层次化嵌入层 (Hierarchical Embeddings)}：融合词、位置和段落 ID 信息。
    \item \textbf{HAT Interleaved 编码器}：包含 6 层 SWE+CSE 交互层，实现局部与全局信息的同步更新。
    \item \textbf{文档分类头 (Document Classifier)}：聚合段向量输出 14 类 Logits。
\end{enumerate}

\begin{figure}[H]
\centering
\includegraphics[width=0.85\textwidth]{../report/graph/model.jpg}
\caption{模型总体架构图}
\label{fig:batch_memory}
\end{figure}

\subsection{动态分段器 (Dynamic Segmenter)}
分段操作集成在 \texttt{HATDataset} 中动态进行。给定重映射后的序列 $x$，目标是将其切分为 $N$ 个长度为 $K=512$ 的 Segment，且总长度控制在 4096 以内（即 $N \le 8$）。

\subsubsection{尾段回拉策略 (Tail Pullback)}
直接切分常导致最后一段极短（如仅 10 个 token），信息密度低且易被 Padding 干扰。我们实现了“尾段回拉”机制：
\begin{itemize}
    \item 若切分后最后一段剩余长度 $r < K/2$，则放弃直接切分，改为从文档末尾向前回溯截取 $K$ 个 token 作为最后一段：
    \begin{equation}
        Seg_{last} = x[L-K : L]
    \end{equation}
    这保证了所有输入的 Segment 均包含充足的语义信息。
\end{itemize}

\subsubsection{长文截断与增强}
\begin{itemize}
    \item \textbf{训练时 (Random Window)}：对超过 8 段的文档，随机选择连续的 8 段进行训练。这不仅解决了显存限制，更作为一种强有力的数据增强手段，提升模型鲁棒性。
    \item \textbf{推理时 (Sliding Window)}：采用滑动窗口策略，将全篇切分为多个窗口（如 [0-7], [4-11]...）分别推理，最后对 Logits 求和/平均。
\end{itemize}

\subsection{层次化嵌入层}
输入张量形状为 $[B, N, K+1]$（模型为每个 Segment 自动插入了 \texttt{[CLS\_SEG]}）。每个 Token 的嵌入表示 $h_0$ 由三部分相加而成：
\begin{equation}
    h_0 = E_{word}[id] + E_{pos\_sw}[pos] + E_{seg\_id}[seg\_idx]
\end{equation}
\begin{itemize}
    \item \textbf{$E_{word}$}：词嵌入，维度 $[7555, 768]$。
    \item \textbf{$E_{pos\_sw}$}：段内位置嵌入，维度 $[513, 768]$，捕捉局部语序。
    \item \textbf{$E_{seg\_id}$}：\textbf{段落 ID 嵌入}，维度 $[8, 768]$。这是 HAT 的关键设计，显式告知模型当前处理的是文档的第几段，增强了模型对篇章结构的感知能力。
\end{itemize}

\subsection{HAT Interleaved 编码器}
我们采用性能最佳的 Interleaved (I1) 布局，共堆叠 6 层。每一层均包含完整的“局部-全局”交互流程：

\subsubsection{Segment-wise Encoder (SWE)}
捕捉局部语义。输入 $H \in \mathbb{R}^{B \times N \times (K+1) \times D}$，将 $B$ 和 $N$ 维度合并，对每个 Segment 独立进行标准 Transformer Self-Attention：
\begin{equation}
    H_{sw} = \text{TransformerBlock}_{local}(H)
\end{equation}

\subsubsection{Cross-segment Encoder (CSE)}
捕捉全局文档上下文。提取每个 Segment 的 \texttt{[CLS\_SEG]} 向量，加上段级位置编码 $E_{pos\_cs}$ 后，在 $N$ 个段向量之间进行 Self-Attention：
\begin{equation}
    CLS_{out} = \text{TransformerBlock}_{global}(CLS_{extracted} + E_{pos\_cs})
\end{equation}
此步骤实现了段落间的信息流动。

\subsubsection{全局信息回注 (Information Injection)}
这是 Interleaved 结构的核心。将 CSE 更新后的全局段向量 $CLS_{out}$ 通过投影层 $W_g$ 映射后，\textbf{广播并加回}到该段内的所有 Token 上：
\begin{equation}
    H_{next}[i, j] = H_{sw}[i, j] + W_g \cdot CLS_{out}[i]
\end{equation}
这使得底层的 Token 表示在每一层都能实时感知到文档级的全局上下文，避免了传统 Hierarchical 模型“先局部后全局”导致的信息割裂。

\subsection{文档分类头}
经过 6 层编码后，我们对所有 Segment 的 \texttt{[CLS\_SEG]} 进行 Masked Mean Pooling 得到文档表示 $h_{doc}$，通过 LayerNorm 和 Dropout 后输入线性层进行 14 分类。

\subsection{模型配置总结}
\begin{table}[h]
    \centering
    \small
    \caption{HAT-Interleaved-512 模型详细配置}
    \begin{tabular}{ll|ll}
        \toprule
        \textbf{参数} & \textbf{数值} & \textbf{参数} & \textbf{数值} \\
        \midrule
        Vocab Size & 7,555 & Layers (SWE+CSE) & 6 \\
        Hidden Size & 768 & Segment Length & 512 \\
        Heads & 12 & Max Segments & 8 \\
        FFN Size & 3072 & Max Context & 4096 \\
        \textbf{总参数量} & \textbf{$\approx$ 95M} & \textbf{结构布局} & \textbf{Interleaved} \\
        \bottomrule
    \end{tabular}
\end{table}

\section{训练与调优}

\subsection{预训练（MLM）}

在正式做分类前，我们先用 Masked Language Modeling（MLM）在新闻语料上进行预训练，让模型先适应任务领域的语义分布。

\paragraph{设置与流程}
\begin{itemize}
    \item 数据来自预处理后的 \texttt{train.csv}，随机打乱后按 9:1 划分训练集和验证集。
    \item 使用 \texttt{HATDataset} 将长文档切成最多 $N$ 个片段，每段长度固定为 $K{=}512$，保证长文本尽量被覆盖。
    \item \texttt{MLMDataCollator} 以 15\% 的概率对 token 进行 Mask，并对 input / label 做 pad，对齐 batch 形状。
\end{itemize}

\paragraph{技术手段}

预训练阶段们基于 HAT-MLM 结构，结合 AdamW 优化器、线性 warmup+衰减学习率、EMA 稳定增强以及周期验证与 checkpoint 机制，
构建了一条稳定可复现的预训练管线，主要配置如下：
\begin{itemize}
    \item 模型采用 HAT-MLM 结构，默认 batch size 为 4，最多训练约 10k 个 step，数据读入、打乱后按 9:1 划分训练集和验证集。
    \item 优化器为 AdamW，学习率 5e-5，weight decay 0.01，\texttt{betas} 设为 (0.9, 0.999)，\texttt{eps} 为 1e-8，并在反向时做梯度裁剪（阈值 1.0）。
    \item 学习率策略为前 6\% 步数线性 warmup，之后线性衰减到 0。
    \item 为提高稳定性，默认启用 EMA（衰减系数 0.999）
\end{itemize}


\paragraph{训练效果}
\begin{table}[h]
    \centering
    \small
    \caption{MLM 训练验证集指标}\label{tab:mlm_metrics}
    \begin{tabular}{cccc}
        \hline
        \textbf{Step} & \textbf{Val Loss} & \textbf{Val PPL} & \textbf{备注} \\
        \hline
        200   & 8.53  & 5064.05 & 起始对照 \\
        1000  & 6.50  & 665.12  & 显著下降 \\
        3000  & 5.60  & 270.06  & 持续收敛 \\
        5000  & 4.08  & 59.42   & 首次低于 60 \\
        7000  & 2.89  & 17.97   & PPL 快速收敛 \\
        9000  & 2.57  & 13.10   & 进入稳定期 \\
        11000 & 2.43  & 11.41   & 收敛尾声 \\
        12000 & \textbf{2.41} & \textbf{11.08} & 最终 / 最佳 \\
        \hline
    \end{tabular}
\end{table}

从表~\ref{tab:mlm_metrics} 可以看出，验证指标从 Step 200 的 PPL 5064 快速下降到 Step 12000 的 11.08，全程未出现反弹；
其保存的最佳模型可直接用于后续分类微调。

\subsection{分类训练 Stage1}

\paragraph{整体流程}
分类阶段采用分层 K 折交叉验证，默认使用 5 折 Stratified K-Fold（种子 42），在保证各折标签分布大致一致的前提下，依次训练并保存每折的最佳模型，用于后续的推理集成。

\paragraph{数据与模型}
我们使用经过预处理的新闻文本与标签作为训练数据，将长文档按固定长度切分为若干片段，并在批处理阶段统一打包成 \([B,N,K]\) 的张量输入模型。分类模型基于 HAT 结构，内部自动添加 CLS 标记并输出文档级 logits；初始化时优先加载对应的 MLM 预训练参数。

\paragraph{损失函数与优化策略}
训练阶段综合使用多种分类损失，包括交叉熵、标签平滑以及 Focal Loss 及其组合。默认设置为标签平滑系数 0.05、Focal 的 $\gamma=2.0$，并配合预计算的类别权重以及可选的加权重采样，共同缓解类别不平衡问题。batch size 默认为 64（训练）/128（验证），并在反向阶段采用梯度裁剪（阈值 1.0）。

优化器选用 AdamW（学习率 3e-5，weight decay 0.01，$\text{betas}=(0.9, 0.999)$，$\text{eps}=1\text{e-}8$），学习率先进行约 6\% 的 warmup，之后线性衰减至 0。默认训练 5 个 epoch，并结合 early stopping，在指标长时间不提升时提前结束训练。

为兼顾效率与稳定性，训练可开启 AMP 混合精度以及 EMA（衰减系数 0.9999）滑动平均，验证阶段优先使用 EMA 权重进行评估。日志按 step 记录，每一折使用独立随机种子，便于稳定复现并在后续对各折最佳 checkpoint 进行集成。```
::contentReference[oaicite:0]{index=0}


\paragraph{验证与模型选择}
每个 epoch 结束后，在当前折的验证集上评估损失、准确率和 macro-F1，以 macro-F1 作为主要指标选取该折的最佳模型；同时结合 early stopping，当若干轮内指标不再提升时提前停止训练，以降低过拟合风险并避免无效计算。

\begin{table}[h]
    \centering
    \small
    \caption{Stage1 K 折验证最佳分数}\label{tab:stage1_metrics}
    \begin{tabular}{ccc}
        \hline
        \textbf{Fold} & \textbf{Best Val Macro-F1} & \textbf{Checkpoint} \\
        \hline
        1 & 0.9597 & \texttt{hat\_cls\_fold0\_best.pt} \\
        2 & 0.9569 & \texttt{hat\_cls\_fold1\_best.pt} \\
        3 & 0.9609 & \texttt{hat\_cls\_fold2\_best.pt} \\
        4 & 0.9612 & \texttt{hat\_cls\_fold3\_best.pt} \\
        5 & 0.9625 & \texttt{hat\_cls\_fold4\_best.pt} \\
        \hline
        \textbf{均值} & \textbf{0.9602 ± 0.0019} & 存于 \texttt{checkpoints/cls\_hat512\_kfold} \\
        \hline
    \end{tabular}
\end{table}

从表~\ref{tab:stage1_metrics} 可以看出，K 折训练共耗时约 946 分钟，各折最佳模型均由 EMA 权重获得，后续推理直接集成以上 5 个 checkpoint。

\subsection{第二阶段微调（Stage2）}

\paragraph{动机与设定}
在一阶段基础上，我们尝试对每折最佳模型做小学习率的二次微调，希望通过更强的正则和轻量数据增强再挖一点性能增益。

\paragraph{训练设置}
\begin{itemize}
    \item 继续使用同一份 K 折划分的数据，以 \texttt{hat\_cls\_fold\{k\}\_best.pt} 为初始权重。
    \item 采用较小学习率的 AdamW（lr 1e-5，weight decay 0.01），训练 1\textasciitilde{}2 个 epoch，仍保留梯度裁剪与 AMP。
    \item 仅在训练阶段开启随机滑窗起点（token 级偏移），增加长文档的“视角多样性”；可选开启 R-Drop 和 FGM 等正则手段。
\end{itemize}

\paragraph{结果与分析}
实际实验中，Stage2 的 macro-F1 与 Stage1 相比整体变化不大，部分折略有下降。最终我们在提交结果时仍主要采用 Stage1 的权重。初步判断原因包括：
\begin{table}[h]
    \centering
    \small
    \caption{Stage2 K 折部分微调结果}\label{tab:stage2_metrics}
    \begin{tabular}{cccc}
        \hline
        \textbf{Fold} & \textbf{Stage2 最佳 F1} & \textbf{Stage1 最佳 F1} & \textbf{是否超过} \\
        \hline
        1 & 0.9572 & 0.9597 & 否 \\
        2 & 0.9541 & 0.9569 & 否 \\
        3 & 0.9565 & 0.9609 & 否 \\
        \hline
    \end{tabular}
\end{table}

从表~\ref{tab:stage2_metrics} 可以看出，整体上 Stage2 未能超越 Stage1，且作业在 Fold 3 的第 2 个 epoch 途中被取消，后续折未运行；最终推理继续沿用 Stage1 的 5 个折模型。
\begin{itemize}
    \item 一阶段训练已经将模型推到较高性能区间，小步长、少步数的二次微调空间有限；
    \item 额外正则（如 R-Drop、对抗训练）在短训练阶段带来的“扰动”大于收益；
    \item 随机滑窗带来的分布轻微漂移，在有限迭代内难以完全重新收敛。
\end{itemize}

\section{推理与部署}

\subsection{整体流程}

推理阶段由 \texttt{infer\_kfold.py} 统一调度：自动收集各折模型（若存在 Stage2 则优先使用 Stage2 的 checkpoint，否则回退为 Stage1 模型），调用 \texttt{infer.py} 完成长文档滑窗、窗口聚合和多模型集成，最终输出提交文件。

\subsection{滑窗与特征构建}
\begin{itemize}
    \item 使用 \texttt{InferenceDataset} 为每条样本保留 \texttt{doc\_id}，并在 collator 中生成对应的滑窗序列。
    \item \texttt{InferenceCollator} 将每个文档的窗口 pad 到统一形状（如 \([N,K]\) = 8×512），再堆叠成批次输入模型。
    \item 支持简单的窗口 TTA（如多种 token offset），为同一文档生成多组窗口，提高鲁棒性。
\end{itemize}

\subsection{集成与决策}

\begin{itemize}
    \item 多折模型统一加载后，先在窗口维度对同一文档的 logits 做聚合（如均值 / 最大值 / 置信度加权均值），得到文档级 logits。
    \item 再在模型维度做集成，可选择按 logits 平均或按 softmax 概率平均，并支持按验证 macro-F1 为各模型加权。
    \item 支持可选阈值策略：二分类可以设置统一阈值，多分类可以为不同类别单独设置阈值；若提供验证集，可通过网格搜索自动调节。
\end{itemize}

\subsection{输出与稳定性}

推理脚本支持在验证集上跑完整链路做一次 sanity check，以确认聚合和阈值设定合理。最终按照天池要求输出提交用 CSV，并可选择额外保存中间 logits 方便后续分析。  
在本任务中，虽然实现了 Stage2 微调，但最终线上结果主要由 Stage1 K 折集成模型贡献，推理流程对是否存在 Stage2 权重具有良好的兼容性。


\section{实验总结与感悟}

这次做长文本分类，最大的收获是“先把链路跑通，再慢慢抠细节”。一开始总想一步到位上所有花活，但显存、时长都把我们掐得死死的。后来改成先用最小可行方案通关，再逐块替换成 HAT、滑窗、集成，反而节奏更稳。

长文本处理的两个坑印象最深：其一是切分策略，末尾片段太短会把信息丢光，Tail Segment Pullback 确实帮了大忙；其二是类别不平衡，class weight + sampler 组合比我们原来只用一种手段稳得多。另一个小教训是 Stage2 微调：理论上应该更好，但短程微调加随机滑窗带来的分布漂移，把收益抵消了，提醒我们“没必要的套路就别硬上”。

调参心得：学习率、warmup 比想象中敏感，尤其是带 EMA 时，稍大的 lr 会导致 EMA 权重“跟不上节奏”，验证集抖动明显；梯度裁剪和 AMP 基本是白送的稳定性提升，可以默认开。日志和 checkpoint 也要勤做，遇到 NaN 能快速回溯。

推理阶段，滑窗聚合 + 多模型加权的组合挺香，但要注意推理时长与提交截止的平衡；我们最后保留了 window TTA 的接口，但默认不开，以免超时。整体体验下来，K 折 + 加权集成是当前资源约束下最稳妥的解。

如果有更多时间，想补的方向有：更长的预训练步数（或更好的预训练数据），尝试对比 Hyena / Mamba 这类长序列模型，和更系统的阈值调优。但至少目前的方案在可靠性和复现性上已经可落地。
\end{document}