\documentclass[a4paper,12pt]{ctexart}
\usepackage[utf8]{inputenc}
\usepackage{geometry}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{hyperref}
\usepackage{float}
\usepackage{listings}
\usepackage{xcolor}

\geometry{left=2.5cm,right=2.5cm,top=2.5cm,bottom=2.5cm}

% 代码高亮设置
\lstset{
    basicstyle=\small\ttfamily,
    columns=flexible,
    breaklines=true,
    backgroundcolor=\color{gray!10},
    frame=single,
    keywordstyle=\color{blue},
    commentstyle=\color{green!50!black},
    stringstyle=\color{red}
}

\title{\textbf{NLP新闻分类学习赛\\实验报告}}
\author{\textbf{队伍名：Attention请注意我} \\ 排名：rk3}
\date{\today}

\begin{document}

\maketitle

\section{团队成员与分工}

本项目采用“共同决策 + 分工落地”的协作机制：在方案设计阶段由两人共同调研、讨论并决策整体技术路线，
在实现阶段按职责拆分任务、各自负责落地。
为在兼顾效率与质量的前提下实现分工清晰、任务解耦，我们将工作划分为前后端两个角色，具体如下：

\begin{itemize}
    \item \textbf{郑亦航}（前端：数据与模型设计）
    \begin{itemize}
        \item 负责数据探索性分析（如文本长度分布、类别不平衡等），撰写分析报告，为后续建模与训练策略提供依据。
        \item 搭建端到端的数据预处理流水线，完成数据清洗、切分、标签重映射等脚本封装，确保全流程可复现。
        \item 设计并实现 HAT 模型基础架构，定义输入管线与关键模块接口，并完成模型骨架的单元测试。
    \end{itemize}

    \item \textbf{李梓煊}（后端：训练与部署优化）
    \begin{itemize}
        \item 实现 MLM 预训练流程，完成与数据管线、Tokenizer / ID 偏移的对接与适配，保证长文本场景下的稳定训练。
        \item 优化分类训练流程，覆盖梯度累积、混合精度训练、学习率调度等环节，在长文本场景下提升收敛速度与性能。
        \item 负责第二阶段微调，设计与调整损失函数（如不同 Loss 组合）。
        \item 实现推理部署：基于 K-fold，将各折最佳 HAT 模型用于长文档滑窗前向与窗口聚合，在此基础上进行多模型加权集成，输出最终预测结果。
    \end{itemize}
\end{itemize}


\section{问题定义}

本次比赛的任务是“新闻文本分类”。
\begin{itemize}
    \item \textbf{输入}：匿名处理后的字符 ID 序列（空格分隔），代表一篇新闻文本。数据经过脱敏处理，无法直接看到原始汉字。
    \item \textbf{输出}：将文本分类到 14 个类别之一（如财经、体育、娱乐等），对应的 Label ID 为 0-13。
    \item \textbf{挑战}：
    \begin{enumerate}
        \item \textbf{长文本结构复杂性}：许多样本长度远超 BERT 模型的 512 token 限制（部分达到 40,000+ token），且关键信息在文本中分布稀疏、跨段依赖长，分段式编码容易遗漏关键语义。
        
        \item \textbf{文本风格与领域差异}：数据来源多样，风格从新闻报道到口语评论均有，语气、表达与话题差异显著，使模型难以学习统一、稳健的特征表示。
        
        \item \textbf{类别极度不平衡}：训练集中类别数量分布长尾显著，最大与最小类别比例高达 42:1，模型在标准训练中容易偏向头部类别。
        
        \item \textbf{数据噪声显著}：存在空文本、重复文本、标签冲突、低质量内容等多种噪声，会影响训练稳定性并降低模型泛化能力。
        
        \item \textbf{计算资源与模型容量限制}：长文本 + 大规模模型导致显存压力大，限制可使用的 batch size、对抗训练策略及模型规模，使训练不够充分。
    \end{enumerate}
    
\end{itemize}

\section{数据分析与预处理}

本章节详细阐述对原始数据集的统计分析过程以及为适配模型而设计的数据清洗流水线。

\subsection{数据特征分析}
我们编写了 `TextLengthAnalyzer` 和 `ClassDistributionAnalyzer` 脚本，对 200,000 条训练样本和 50,000 条测试样本进行了详尽的统计分析。

\subsubsection{文本长度分布}
分析结果显示，文本长度呈现显著的长尾分布。
\begin{itemize}
    \item \textbf{统计数据}：训练集文本平均长度（Mean）约为 907 tokens，中位数（Median）为 676。P90 分位点达到 1,796，P95 为 2,457，P99 更是高达 4,228，最长文本甚至接近 58,000 tokens。
    \item \textbf{覆盖率分析}：若采用传统 BERT 的 512 长度截断，将丢失绝大部分样本的尾部信息。统计表明，当截断长度设定为 4096 时，Token 覆盖率可达 97\%，超过 10k tokens 的极端长文占比仅为 0.08\%。
    \item \textbf{结论}：4096 是一个兼顾覆盖率与计算开销的理想上下文窗口长度。
\end{itemize}

\subsubsection{类别不平衡与词表}
数据共包含 14 个类别，类别分布极不平衡，最大类与最小类的样本比例约为 43:1。词汇表（Vocab）总规模约为 6,977，Token ID 范围为 0-7549，未登录词（OOV）极低。这提示我们需要在 Loss 函数层面引入类别平衡策略。

\subsection{预处理流水线}
基于上述分析，我们实施了以下预处理步骤。值得注意的是，数据分段操作并未在此阶段进行，预处理阶段仅输出连续的 Token 序列，分段逻辑留待模型输入层动态处理。

\subsubsection{特殊 Token 重映射}
原始数据的 Token ID 占用了 0-4 的数值空间。为了适配 Transformer 架构所需的特殊标记，我们将所有原始 Token ID 整体偏移 +5：
\begin{equation}
    t' = t_{raw} + 5, \quad t_{raw} \in [0, 7549]
\end{equation}
腾出的空间用于定义：0: \texttt{[PAD]}, 1: \texttt{[UNK]}, 2: \texttt{[CLS\_SEG]}, 3: \texttt{[SEP]}, 4: \texttt{[MASK]}。重映射后的 Vocab Size 约为 7,555。

\subsubsection{数据清洗}
使用 `DataCleaner` 模块移除了 89 条完全重复的文本和 16 条标签冲突的样本，并去除了空文本，以保证训练数据的纯净度。


\section{模型设计}

针对长文档分类任务，单纯截断或使用稀疏注意力（Longformer）虽能处理长文，但在捕捉层级结构方面略显不足。Chalkidis 等人的研究表明，分层 Transformer（HAT）在保持计算高效的同时，性能往往优于纯稀疏注意力模型。
因此，我们设计了 HAT-Interleaved 512 模型作为主力架构，参数量约 90-100M，旨在充分利用 4 x H800 的算力，原生支持 4096 tokens 的上下文。

\subsection{总体架构概览}
模型整体采用“分段-交互-聚合”的设计思路，主要包含四个模块：
\begin{enumerate}
    \item \textbf{分段器 (Document Segmenter)}：负责动态切分长文。
    \item \textbf{嵌入层 (Embedding Layer)}：融合词、位置和段落信息。
    \item \textbf{HAT Interleaved 编码器}：核心模块，包含 6 层 SWE+CSE 交互层。
    \item \textbf{文档分类头 (Classifier)}：输出 14 类 Logits。
\end{enumerate}

\subsection{动态分段器}
分段操作在模型输入流水线中动态进行。给定重映射后的 Token 序列 $x = [t_1, \dots, t_L]$，目标是将其切分为 $N$ 个长度为 $K=512$ 的 Segment，且总长度控制在 4096 以内（即 $N \le 8$）。

\subsubsection{尾段回拉策略}
直接切分可能导致最后一个 Segment 极短（例如仅剩 10 个 token），包含信息量过少且容易被 Padding 淹没。我们实现了“尾段回拉”机制：
\begin{itemize}
    \item 若最后一段剩余长度 $r < K/2$，则放弃直接切分，改为从文档末尾向前回溯截取 $K$ 个 token 作为最后一段：
    \begin{equation}
        Seg_{last} = x[L-K : L]
    \end{equation}
\end{itemize}

\subsubsection{长文截断策略}
\begin{itemize}
    \item \textbf{训练时}：对超过 8 段的文档，采用随机窗口（Random Window）策略，随机选择连续的 8 段进行训练，起到数据增强的作用。
    \item \textbf{推理时}：采用滑动窗口（Sliding Window），将全篇切分为多个窗口分别推理，最后对 Logits 求平均。
\end{itemize}

\subsection{层次化嵌入层}
输入张量形状为 $[B, N, K+1]$（$K+1$ 是因为模型为每个 Segment 插入了专用的$`CLS_{SEG}`$)。每个 Token 的嵌入表示 $h_0$ 由三部分相加而成：
\begin{equation}
    h_0 = E_{word}[id] + E_{pos\_sw}[pos] + E_{seg\_id}[seg\_idx]
\end{equation}
\begin{itemize}
    \item $E_{word}$: 词嵌入，维度 $[7555, 768]$。
    \item $E_{pos\_sw}$: 段内位置嵌入，维度 $[513, 768]$。
    \item $E_{seg\_id}$: 段落 ID 嵌入，维度 $[8, 768]$。这一项帮助模型区分当前处理的是文档的第几段，对捕捉篇章结构至关重要。
\end{itemize}

\subsection{HAT Interleaved 编码器}
不同于先处理完所有局部特征再做全局聚合的级联结构，Interleaved (I1) 布局在每一层都进行局部与全局的交互。编码器共 6 层，每层包含以下三个步骤：

\subsubsection{Segment-wise Encoder (SWE)}
对每个 Segment 独立进行 Self-Attention，捕捉局部语义。输入形状 $H \in \mathbb{R}^{B \times N \times (K+1) \times D}$：
\begin{equation}
    H_{sw} = \text{TransformerBlock}_{shared}(H)
\end{equation}

\subsubsection{Cross-segment Encoder (CSE)}
提取每个 Segment 的 `CLS` 向量，加上段级位置编码 $E_{pos\_cs}$ 后，在 $N$ 个段向量之间进行 Self-Attention：
\begin{equation}
    CLS_{out} = \text{TransformerBlock}(CLS_{in} + E_{pos\_cs})
\end{equation}
这一步实现了跨段落的全局信息交互。

\subsubsection{全局信息回注 (Information Injection)}
这是 Interleaved 结构的关键。将 CSE 更新后的全局段向量 $CLS_{out}$ 通过投影层 $W_g$ 映射后，广播（Broadcast）并加回到该段内的所有 Token 上：
\begin{equation}
    H_{next}[i, j] = H_{sw}[i, j] + W_g \cdot CLS_{out}[i]
\end{equation}
这使得底层的 Token 表示也能实时感知到文档级的全局上下文。

\subsection{分类头与超参}
最终使用最后一层的段向量均值作为文档表示 $h_{doc}$，通过 LayerNorm 和 Dropout 后输入线性层分类。
模型核心超参如下：$d_{model}=768$, $d_{ff}=3072$, $n_{heads}=12$, Layers=6 (SWE+CSE), 总参数量 $\approx 100M$。这种配置在保证长文处理能力的同时，与 BERT-Base 规模相当，训练与推理效率极高。

\section{训练与调优}

\subsection{预训练（MLM）}

在正式做分类前，我们先用 Masked Language Modeling（MLM）在新闻语料上进行预训练，让模型先适应任务领域的语义分布。

\paragraph{设置与流程}
\begin{itemize}
    \item 数据来自预处理后的 \texttt{train.csv}，随机打乱后按 9:1 划分训练集和验证集。
    \item 使用 \texttt{HATDataset} 将长文档切成最多 $N$ 个片段，每段长度固定为 $K{=}512$，保证长文本尽量被覆盖。
    \item \texttt{MLMDataCollator} 以 15\% 的概率对 token 进行 Mask，并对 input / label 做 pad，对齐 batch 形状。
\end{itemize}

\paragraph{技术手段}

预训练阶段们基于 HAT-MLM 结构，结合 AdamW 优化器、线性 warmup+衰减学习率、EMA 稳定增强以及周期验证与 checkpoint 机制，
构建了一条稳定可复现的预训练管线，主要配置如下：
\begin{itemize}
    \item 模型采用 HAT-MLM 结构，默认 batch size 为 4，最多训练约 10k 个 step，数据读入、打乱后按 9:1 划分训练集和验证集。
    \item 优化器为 AdamW，学习率 5e-5，weight decay 0.01，\texttt{betas} 设为 (0.9, 0.999)，\texttt{eps} 为 1e-8，并在反向时做梯度裁剪（阈值 1.0）。
    \item 学习率策略为前 6\% 步数线性 warmup，之后线性衰减到 0。
    \item 为提高稳定性，默认启用 EMA（衰减系数 0.999）
\end{itemize}


\paragraph{训练效果}
\begin{table}[h]
    \centering
    \small
    \begin{tabular}{cccc}
        \hline
        \textbf{Step} & \textbf{Val Loss} & \textbf{Val PPL} & \textbf{备注} \\
        \hline
        200   & 8.53  & 5064.05 & 起始对照 \\
        1000  & 6.50  & 665.12  & 显著下降 \\
        3000  & 5.60  & 270.06  & 持续收敛 \\
        5000  & 4.08  & 59.42   & 首次低于 60 \\
        7000  & 2.89  & 17.97   & PPL 快速收敛 \\
        9000  & 2.57  & 13.10   & 进入稳定期 \\
        11000 & 2.43  & 11.41   & 收敛尾声 \\
        12000 & \textbf{2.41} & \textbf{11.08} & 最终 / 最佳 \\
        \hline
    \end{tabular}
    \caption{MLM 训练验证集指标}\label{tab:mlm_metrics}
\end{table}

从表~\ref{tab:mlm_metrics} 可以看出，验证指标从 Step 200 的 PPL 5064 快速下降到 Step 12000 的 11.08，全程未出现反弹；
其保存的最佳模型可直接用于后续分类微调。

\subsection{分类训练 Stage1}

\paragraph{整体流程}
分类阶段采用分层 K 折交叉验证，默认使用 5 折 Stratified K-Fold（种子 42），在保证各折标签分布大致一致的前提下，依次训练并保存每折的最佳模型，用于后续的推理集成。

\paragraph{数据与模型}
我们使用经过预处理的新闻文本与标签作为训练数据，将长文档按固定长度切分为若干片段，并在批处理阶段统一打包成 \([B,N,K]\) 的张量输入模型。分类模型基于 HAT 结构，内部自动添加 CLS 标记并输出文档级 logits；初始化时优先加载对应的 MLM 预训练参数。

\paragraph{损失函数与优化策略}
训练阶段综合使用多种分类损失，包括交叉熵、标签平滑以及 Focal Loss 及其组合。默认设置为标签平滑系数 0.05、Focal 的 $\gamma=2.0$，并配合预计算的类别权重以及可选的加权重采样，共同缓解类别不平衡问题。batch size 默认为 64（训练）/128（验证），并在反向阶段采用梯度裁剪（阈值 1.0）。

优化器选用 AdamW（学习率 3e-5，weight decay 0.01，$\text{betas}=(0.9, 0.999)$，$\text{eps}=1\text{e-}8$），学习率先进行约 6\% 的 warmup，之后线性衰减至 0。默认训练 5 个 epoch，并结合 early stopping，在指标长时间不提升时提前结束训练。

为兼顾效率与稳定性，训练可开启 AMP 混合精度以及 EMA（衰减系数 0.9999）滑动平均，验证阶段优先使用 EMA 权重进行评估。日志按 step 记录，每一折使用独立随机种子，便于稳定复现并在后续对各折最佳 checkpoint 进行集成。```
::contentReference[oaicite:0]{index=0}


\paragraph{验证与模型选择}
每个 epoch 结束后，在当前折的验证集上评估损失、准确率和 macro-F1，以 macro-F1 作为主要指标选取该折的最佳模型；同时结合 early stopping，当若干轮内指标不再提升时提前停止训练，以降低过拟合风险并避免无效计算。

\begin{table}[h]
    \centering
    \small
    \begin{tabular}{ccc}
        \hline
        \textbf{Fold} & \textbf{Best Val Macro-F1} & \textbf{Checkpoint} \\
        \hline
        1 & 0.9597 & \texttt{hat\_cls\_fold0\_best.pt} \\
        2 & 0.9569 & \texttt{hat\_cls\_fold1\_best.pt} \\
        3 & 0.9609 & \texttt{hat\_cls\_fold2\_best.pt} \\
        4 & 0.9612 & \texttt{hat\_cls\_fold3\_best.pt} \\
        5 & 0.9625 & \texttt{hat\_cls\_fold4\_best.pt} \\
        \hline
        \textbf{均值} & \textbf{0.9602 ± 0.0019} & 存于 \texttt{checkpoints/cls\_hat512\_kfold} \\
        \hline
    \end{tabular}
    \caption{Stage1 K 折验证最佳分数}\label{tab:stage1_metrics}
\end{table}

从表~\ref{tab:stage1_metrics} 可以看出，K 折训练共耗时约 946 分钟，各折最佳模型均由 EMA 权重获得，后续推理直接集成以上 5 个 checkpoint。

\subsection{第二阶段微调（Stage2）}

\paragraph{动机与设定}
在一阶段基础上，我们尝试对每折最佳模型做小学习率的二次微调，希望通过更强的正则和轻量数据增强再挖一点性能增益。

\paragraph{训练设置}
\begin{itemize}
    \item 继续使用同一份 K 折划分的数据，以 \texttt{hat\_cls\_fold\{k\}\_best.pt} 为初始权重。
    \item 采用较小学习率的 AdamW（lr 1e-5，weight decay 0.01），训练 1\textasciitilde{}2 个 epoch，仍保留梯度裁剪与 AMP。
    \item 仅在训练阶段开启随机滑窗起点（token 级偏移），增加长文档的“视角多样性”；可选开启 R-Drop 和 FGM 等正则手段。
\end{itemize}

\paragraph{结果与分析}
实际实验中，Stage2 的 macro-F1 与 Stage1 相比整体变化不大，部分折略有下降。最终我们在提交结果时仍主要采用 Stage1 的权重。初步判断原因包括：
\begin{table}[h]
    \centering
    \small
    \begin{tabular}{cccc}
        \hline
        \textbf{Fold} & \textbf{Stage2 最佳 F1} & \textbf{Stage1 最佳 F1} & \textbf{是否超过} \\
        \hline
        1 & 0.9572 & 0.9597 & 否 \\
        2 & 0.9541 & 0.9569 & 否 \\
        3 & 0.9565 & 0.9609 & 否 \\
        \hline
    \end{tabular}
    \caption{Stage2 K 折部分微调结果}\label{tab:stage2_metrics}
\end{table}

从表~\ref{tab:stage2_metrics} 可以看出，整体上 Stage2 未能超越 Stage1，且作业在 Fold 3 的第 2 个 epoch 途中被取消，后续折未运行；最终推理继续沿用 Stage1 的 5 个折模型。
\begin{itemize}
    \item 一阶段训练已经将模型推到较高性能区间，小步长、少步数的二次微调空间有限；
    \item 额外正则（如 R-Drop、对抗训练）在短训练阶段带来的“扰动”大于收益；
    \item 随机滑窗带来的分布轻微漂移，在有限迭代内难以完全重新收敛。
\end{itemize}

\section{推理与部署}

\subsection{整体流程}

推理阶段由 \texttt{infer\_kfold.py} 统一调度：自动收集各折模型（若存在 Stage2 则优先使用 Stage2 的 checkpoint，否则回退为 Stage1 模型），调用 \texttt{infer.py} 完成长文档滑窗、窗口聚合和多模型集成，最终输出提交文件。

\subsection{滑窗与特征构建}
\begin{itemize}
    \item 使用 \texttt{InferenceDataset} 为每条样本保留 \texttt{doc\_id}，并在 collator 中生成对应的滑窗序列。
    \item \texttt{InferenceCollator} 将每个文档的窗口 pad 到统一形状（如 \([N,K]\) = 8×512），再堆叠成批次输入模型。
    \item 支持简单的窗口 TTA（如多种 token offset），为同一文档生成多组窗口，提高鲁棒性。
\end{itemize}

\subsection{集成与决策}

\begin{itemize}
    \item 多折模型统一加载后，先在窗口维度对同一文档的 logits 做聚合（如均值 / 最大值 / 置信度加权均值），得到文档级 logits。
    \item 再在模型维度做集成，可选择按 logits 平均或按 softmax 概率平均，并支持按验证 macro-F1 为各模型加权。
    \item 支持可选阈值策略：二分类可以设置统一阈值，多分类可以为不同类别单独设置阈值；若提供验证集，可通过网格搜索自动调节。
\end{itemize}

\subsection{输出与稳定性}

推理脚本支持在验证集上跑完整链路做一次 sanity check，以确认聚合和阈值设定合理。最终按照天池要求输出提交用 CSV，并可选择额外保存中间 logits 方便后续分析。  
在本任务中，虽然实现了 Stage2 微调，但最终线上结果主要由 Stage1 K 折集成模型贡献，推理流程对是否存在 Stage2 权重具有良好的兼容性。


\section{实验总结与感悟}

这次做长文本分类，最大的收获是“先把链路跑通，再慢慢抠细节”。一开始总想一步到位上所有花活，但显存、时长都把我们掐得死死的。后来改成先用最小可行方案通关，再逐块替换成 HAT、滑窗、集成，反而节奏更稳。

长文本处理的两个坑印象最深：其一是切分策略，末尾片段太短会把信息丢光，Tail Segment Pullback 确实帮了大忙；其二是类别不平衡，class weight + sampler 组合比我们原来只用一种手段稳得多。另一个小教训是 Stage2 微调：理论上应该更好，但短程微调加随机滑窗带来的分布漂移，把收益抵消了，提醒我们“没必要的套路就别硬上”。

调参心得：学习率、warmup 比想象中敏感，尤其是带 EMA 时，稍大的 lr 会导致 EMA 权重“跟不上节奏”，验证集抖动明显；梯度裁剪和 AMP 基本是白送的稳定性提升，可以默认开。日志和 checkpoint 也要勤做，遇到 NaN 能快速回溯。

推理阶段，滑窗聚合 + 多模型加权的组合挺香，但要注意推理时长与提交截止的平衡；我们最后保留了 window TTA 的接口，但默认不开，以免超时。整体体验下来，K 折 + 加权集成是当前资源约束下最稳妥的解。

如果有更多时间，想补的方向有：更长的预训练步数（或更好的预训练数据），尝试对比 Hyena / Mamba 这类长序列模型，和更系统的阈值调优。但至少目前的方案在可靠性和复现性上已经可落地。
\end{document}