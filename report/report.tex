\documentclass[a4paper,12pt]{ctexart}
\usepackage[utf8]{inputenc}
\usepackage{geometry}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{hyperref}
\usepackage{float}
\usepackage{listings}
\usepackage{xcolor}

\geometry{left=2.5cm,right=2.5cm,top=2.5cm,bottom=2.5cm}

% 代码高亮设置
\lstset{
    basicstyle=\small\ttfamily,
    columns=flexible,
    breaklines=true,
    backgroundcolor=\color{gray!10},
    frame=single,
    keywordstyle=\color{blue},
    commentstyle=\color{green!50!black},
    stringstyle=\color{red}
}

\title{\textbf{NLP新闻分类学习赛\\实验报告}}
\author{\textbf{队伍名：Attention请注意我} \\ 排名：rk3}
\date{\today}

\begin{document}

\maketitle

\section{团队成员与分工}

本项目采用“共同决策 + 分工落地”的协作机制：在方案设计阶段由两人共同调研、讨论并决策整体技术路线，
在实现阶段按职责拆分任务、各自负责落地。
为在兼顾效率与质量的前提下实现分工清晰、任务解耦，我们将工作划分为前后端两个角色，具体如下：

\begin{itemize}
    \item \textbf{郑亦航}（前端：数据与模型设计）
    \begin{itemize}
        \item 负责数据探索性分析（如文本长度分布、类别不平衡等），撰写分析报告，为后续建模与训练策略提供依据。
        \item 搭建端到端的数据预处理流水线，完成数据清洗、切分、标签重映射等脚本封装，确保全流程可复现。
        \item 设计并实现 HAT 模型基础架构，定义输入管线与关键模块接口，并完成模型骨架的单元测试。
    \end{itemize}

    \item \textbf{李梓煊}（后端：训练与部署优化）
    \begin{itemize}
        \item 实现 MLM 预训练流程，完成与数据管线、Tokenizer / ID 偏移的对接与适配，保证长文本场景下的稳定训练。
        \item 优化分类训练流程，覆盖梯度累积、混合精度训练、学习率调度等环节，在长文本场景下提升收敛速度与性能。
        \item 负责第二阶段微调，设计与调整损失函数（如不同 Loss 组合）。
        \item 实现推理部署：基于 K-fold，将各折最佳 HAT 模型用于长文档滑窗前向与窗口聚合，在此基础上进行多模型加权集成，输出最终预测结果。
    \end{itemize}
\end{itemize}


\section{问题定义}

本次比赛的任务是“新闻文本分类”。
\begin{itemize}
    \item \textbf{输入}：匿名处理后的字符 ID 序列（空格分隔），代表一篇新闻文本。数据经过脱敏处理，无法直接看到原始汉字。
    \item \textbf{输出}：将文本分类到 14 个类别之一（如财经、体育、娱乐等），对应的 Label ID 为 0-13。
    \item \textbf{挑战}：
    \begin{enumerate}
        \item \textbf{长文本结构复杂性}：许多样本长度远超 BERT 模型的 512 token 限制（部分达到 40,000+ token），且关键信息在文本中分布稀疏、跨段依赖长，分段式编码容易遗漏关键语义。
        
        \item \textbf{文本风格与领域差异}：数据来源多样，风格从新闻报道到口语评论均有，语气、表达与话题差异显著，使模型难以学习统一、稳健的特征表示。
        
        \item \textbf{类别极度不平衡}：训练集中类别数量分布长尾显著，最大与最小类别比例高达 42:1，模型在标准训练中容易偏向头部类别。
        
        \item \textbf{数据噪声显著}：存在空文本、重复文本、标签冲突、低质量内容等多种噪声，会影响训练稳定性并降低模型泛化能力。
        
        \item \textbf{计算资源与模型容量限制}：长文本 + 大规模模型导致显存压力大，限制可使用的 batch size、对抗训练策略及模型规模，使训练不够充分。
    \end{enumerate}
    
\end{itemize}

\section{数据分析与预处理}

\subsection{特征分析与清洗}
通过编写分析脚本，我们发现原始数据存在以下问题并进行了处理：
\begin{enumerate}
    \item \textbf{去重与冲突处理}：移除了完全重复的文本（89条）和标签冲突的样本（16条）。
    \item \textbf{长度分布}：平均长度约 900 tokens，远超 512。因此直接截断会导致严重信息丢失，必须采用长文本建模方案。
    \item \textbf{类别分布}：Label 0-3 占据主要部分，Label 13 仅有 0.45\%。我们计算了 \texttt{inverse\_sqrt} 类别权重用于 Loss 计算。
\end{enumerate}

\subsection{从原始数据到模型输入的转化过程}

\begin{enumerate}
    \item \textbf{Token ID 重映射 (Remap)}：
    原始数据的 ID 范围是 0-7549。为了给特殊 Token 预留位置，我们将所有原始 ID 偏移 +5。
    \[ ID_{new} = ID_{raw} + 5 \]
    预留位置：0=[PAD], 1=[UNK], 2=[CLS\_SEG], 3=[SEP], 4=[MASK]。

    \item \textbf{分段 (Segmentation)}：
    由于选择了层次化模型，我们需要将长文档切分为 $N$ 个片段。设定每个片段长度 $K=512$，最大片段数 $N=8$。
    \begin{itemize}
        \item 若文档长度 $L > 4096$，则截断。
        \item 若文档长度 $L < 4096$，则填充 [PAD]。
        \item \textbf{Tail Segment Pullback}：为了防止最后一个片段过短（如仅有几个 token），如果末尾片段有效长度不足阈值，我们从文档末尾向前回溯采样，保证最后一个片段也是语义丰富的。
    \end{itemize}

    \item \textbf{模型输入形状}：
    经过 `HATDataset` 处理后，一个样本被转换为张量形状：
    \[ \text{Input IDs}: [N, K] \rightarrow [8, 512] \]
    模型内部会自动添加 `[CLS]` token，最终计算形状为 $[8, 513]$。
\end{enumerate}

\section{模型设计}

鉴于文本极长的特性，传统的 BERT 模型（最大长度 512）无法处理，而 Longformer 等模型显存占用过大。经过广泛的调研后，我们选择了 \textbf{HAT (Hierarchical Attention Transformer)} 架构，具体配置为 \texttt{HAT-Interleaved-512}。

\subsection{模型选择依据}
HAT 模型通过分层机制解决长文本问题：
\begin{itemize}
    \item \textbf{计算效率}：将注意力计算复杂度从 $O(L^2)$ 降低到 $O(N \cdot K^2 + N^2)$。
    \item \textbf{全局信息交互}：通过 Segment-wise 和 Cross-segment 的交替结构，既捕捉局部细节，又捕捉全文脉络。
\end{itemize}

\subsection{模型原理}
我们的模型由 6 层 `HATLayer` 堆叠而成。每一层包含三个步骤：

\begin{enumerate}
    \item \textbf{Segment-wise Encoder (SWE)}：
    对每个 512 长度的片段独立进行 Self-Attention。
    \[ \text{SWE}(S_i) = \text{TransformerBlock}(S_i) \]
    这一步捕捉片段内的局部语义。

    \item \textbf{Cross-segment Encoder (CSE)}：
    收集所有片段的 `[CLS]` token，进行一次全局 Self-Attention。
    \[ H_{cls} = \text{Concat}([CLS]_1, [CLS]_2, ..., [CLS]_N) \]
    \[ H'_{cls} = \text{TransformerBlock}(H_{cls}) \]
    这一步让不同片段的信息进行交互。

    \item \textbf{Information Injection}：
    将交互后的全局信息 $H'_{cls}$ 重新广播（Broadcast）并加回到各片段的 token 表示中，实现全局上下文对局部的指导。
\end{enumerate}

\subsection{输出形式}
模型的最终输出取首个片段的 `[CLS]` 向量（或所有片段 `[CLS]` 的池化），通过一个线性层映射到 14 维空间：
\[ \text{Logits} = \text{Linear}(H_{final\_cls}) \in \mathbb{R}^{14} \]

\section{训练与调优}

\subsection{预训练（MLM）}

在正式做分类前，我们先用 Masked Language Modeling（MLM）在新闻语料上进行预训练，让模型先适应任务领域的语义分布。

\paragraph{设置与流程}
\begin{itemize}
    \item 数据来自预处理后的 \texttt{train.csv}，随机打乱后按 9:1 划分训练集和验证集。
    \item 使用 \texttt{HATDataset} 将长文档切成最多 $N$ 个片段，每段长度固定为 $K{=}512$，保证长文本尽量被覆盖。
    \item \texttt{MLMDataCollator} 以 15\% 的概率对 token 进行 Mask，并对 input / label 做 pad，对齐 batch 形状。
    \item 模型采用 HAT-MLM 结构，优化器为 AdamW（lr 5e-5，weight decay 0.01），线性 warmup + 线性衰减，梯度裁剪阈值 1.0，训练步数上限约 10k。
\end{itemize}

\paragraph{训练效果}
\begin{table}[h]
    \centering
    \small
    \begin{tabular}{cccc}
        \hline
        \textbf{Step} & \textbf{Val Loss} & \textbf{Val PPL} & \textbf{备注} \\
        \hline
        200   & 8.53  & 5064.05 & 起始对照 \\
        1000  & 6.50  & 665.12  & 显著下降 \\
        3000  & 5.60  & 270.06  & 持续收敛 \\
        5000  & 4.08  & 59.42   & 首次低于 60 \\
        7000  & 2.89  & 17.97   & PPL 快速收敛 \\
        9000  & 2.57  & 13.10   & 进入稳定期 \\
        11000 & 2.43  & 11.41   & 收敛尾声 \\
        12000 & \textbf{2.41} & \textbf{11.08} & 最终 / 最佳 \\
        \hline
    \end{tabular}
    \caption{MLM 训练验证集指标（摘自 \texttt{slurm-65750.out}）}
\end{table}

验证指标从 Step 200 的 PPL 5064 快速下降到 Step 12000 的 11.08，全程未出现反弹；总耗时约 324 分钟，平均 9.9 samples/s，最终模型已保存为 \texttt{checkpoints/mlm\_hat512/hat\_mlm\_final.pt}，可直接用于后续分类微调。

\subsection{分类训练（一阶段）}

\paragraph{整体流程}
分类阶段采用分层 K 折交叉验证。默认使用 5 折 Stratified K-Fold（种子 42），在保证各折标签分布大致一致的前提下，依次训练并保存每折的最佳模型 \texttt{hat\_cls\_fold\{k\}\_best.pt}，后续用于集成。

\paragraph{数据与模型}
\begin{itemize}
    \item 训练数据来自 \texttt{data/processed/train.csv}（文本 + 标签），通过 \texttt{HATDataset} 将长文档切成最多 $N$ 段、每段 $K{=}512$，输出形状为 \([N,K]\)。
    \item \texttt{HATDataCollator} 将样本打包为 \([B,N,K]\) 的 \texttt{input\_ids} / \texttt{attention\_mask} 和 \([B]\) 的标签。
    \item 分类模型为 \texttt{HATInterleaved512ForClassification}，内部自动添加 CLS，并输出 logits；优先加载 MLM 预训练权重 \texttt{hat\_mlm\_final.pt}，若未提供则随机初始化。
\end{itemize}

\paragraph{损失函数与优化策略}
\begin{itemize}
    \item 损失函数通过 \texttt{create\_loss\_fn} 选择，包括交叉熵（\texttt{ce}）、标签平滑（\texttt{smooth}，默认，平滑系数 0.05）、Focal Loss 及其组合（\texttt{focal} / \texttt{focal\_smooth}，gamma=2.0）。
    \item 类别权重从 \texttt{class\_weights.npy} 加载，用于缓解类别不平衡；可选 \texttt{WeightedRandomSampler} 进一步平衡长尾。
    \item 优化器为 AdamW（lr 3e-5，weight decay 0.01），配合线性 warmup + 线性衰减，默认训练 5 个 epoch；支持 AMP 混合精度和 EMA（decay 0.9999），验证时优先使用 EMA 权重。
\end{itemize}

\paragraph{验证与模型选择}
每个 epoch 结束后，在当前折的验证集上评估 loss、accuracy 和 macro-F1，并以 macro-F1 作为主指标选取最优权重，保存为 \texttt{hat\_cls\_fold\{k\}\_best.pt}。同时启用 early stopping，若若干个 epoch 指标无提升则提前停止，避免过拟合和无效训练。

\begin{table}[h]
    \centering
    \small
    \begin{tabular}{ccc}
        \hline
        \textbf{Fold} & \textbf{Best Val Macro-F1} & \textbf{Checkpoint} \\
        \hline
        1 & 0.9597 & \texttt{hat\_cls\_fold0\_best.pt} \\
        2 & 0.9569 & \texttt{hat\_cls\_fold1\_best.pt} \\
        3 & 0.9609 & \texttt{hat\_cls\_fold2\_best.pt} \\
        4 & 0.9612 & \texttt{hat\_cls\_fold3\_best.pt} \\
        5 & 0.9625 & \texttt{hat\_cls\_fold4\_best.pt} \\
        \hline
        \textbf{均值} & \textbf{0.9602 ± 0.0019} & 全部存于 \texttt{checkpoints/cls\_hat512\_kfold} \\
        \hline
    \end{tabular}
    \caption{Stage1 K 折验证最佳分数（摘自 \texttt{slurm-kfold-66070.out}）}
\end{table}

K 折训练共耗时约 946 分钟，各折最佳模型均由 EMA 权重获得，后续推理直接集成以上 5 个 checkpoint。

\subsection{第二阶段微调（Stage2）}

\paragraph{动机与设定}
在一阶段基础上，我们尝试对每折最佳模型做小学习率的二次微调，希望通过更强的正则和轻量数据增强再挖一点性能增益。

\paragraph{训练设置}
\begin{itemize}
    \item 继续使用同一份 K 折划分的数据，以 \texttt{hat\_cls\_fold\{k\}\_best.pt} 为初始权重。
    \item 采用较小学习率的 AdamW（lr 1e-5，weight decay 0.01），训练 1\textasciitilde{}2 个 epoch，仍保留梯度裁剪与 AMP。
    \item 仅在训练阶段开启随机滑窗起点（token 级偏移），增加长文档的“视角多样性”；可选开启 R-Drop 和 FGM 等正则手段。
\end{itemize}

\paragraph{结果与分析}
实际实验中，Stage2 的 macro-F1 与 Stage1 相比整体变化不大，部分折略有下降。最终我们在提交结果时仍主要采用 Stage1 的权重。初步判断原因包括：
\begin{table}[h]
    \centering
    \small
    \begin{tabular}{cccc}
        \hline
        \textbf{Fold} & \textbf{Stage2 最佳 F1} & \textbf{Stage1 最佳 F1} & \textbf{是否超过} \\
        \hline
        1 & 0.9572 & 0.9597 & 否 \\
        2 & 0.9541 & 0.9569 & 否 \\
        3 & 0.9565 & 0.9609 & 否 \\
        \hline
    \end{tabular}
    \caption{Stage2 K 折部分微调结果（摘自 \texttt{slurm-kfold-stage2-66428.out}）}
\end{table}

整体上 Stage2 未能超越 Stage1，且作业在 Fold 3 的第 2 个 epoch 途中被取消，后续折未运行；最终推理继续沿用 Stage1 的 5 个折模型。
\begin{itemize}
    \item 一阶段训练已经将模型推到较高性能区间，小步长、少步数的二次微调空间有限；
    \item 额外正则（如 R-Drop、对抗训练）在短训练阶段带来的“扰动”大于收益；
    \item 随机滑窗带来的分布轻微漂移，在有限迭代内难以完全重新收敛。
\end{itemize}

\section{推理与部署}

\subsection{整体流程}

推理阶段由 \texttt{infer\_kfold.py} 统一调度：自动收集各折模型（若存在 Stage2 则优先使用 \texttt{hat\_cls\_fold\{k\}\_stage2\_best.pt}，否则回退为 Stage1 模型），调用 \texttt{infer.py} 完成长文档滑窗、窗口聚合和多模型集成，最终输出提交文件。

\subsection{滑窗与特征构建}
\begin{itemize}
    \item 使用 \texttt{InferenceDataset} 为每条样本保留 \texttt{doc\_id}，并在 collator 中生成对应的滑窗序列。
    \item \texttt{InferenceCollator} 将每个文档的窗口 pad 到统一形状（如 \([N,K]\) = 8×512），再堆叠成批次输入模型。
    \item 支持简单的窗口 TTA（如多种 token offset），为同一文档生成多组窗口，提高鲁棒性。
\end{itemize}

\subsection{集成与决策}

\begin{itemize}
    \item 多折模型统一加载后，先在窗口维度对同一文档的 logits 做聚合（如均值 / 最大值 / 置信度加权均值），得到文档级 logits。
    \item 再在模型维度做集成，可选择按 logits 平均或按 softmax 概率平均，并支持按验证 macro-F1 为各模型加权。
    \item 支持可选阈值策略：二分类可以设置统一阈值，多分类可以为不同类别单独设置阈值；若提供验证集，可通过网格搜索自动调节。
\end{itemize}

\subsection{输出与稳定性}

推理脚本支持在验证集上跑完整链路做一次 sanity check，以确认聚合和阈值设定合理。最终按照天池要求输出提交用 CSV，并可选择额外保存中间 logits 方便后续分析。  
在本任务中，虽然实现了 Stage2 微调，但最终线上结果主要由 Stage1 K 折集成模型贡献，推理流程对是否存在 Stage2 权重具有良好的兼容性。


\section{实验总结与感悟}

这次做长文本分类，最大的收获是“先把链路跑通，再慢慢抠细节”。一开始总想一步到位上所有花活，但显存、时长都把我们掐得死死的。后来改成先用最小可行方案通关，再逐块替换成 HAT、滑窗、集成，反而节奏更稳。

长文本处理的两个坑印象最深：其一是切分策略，末尾片段太短会把信息丢光，Tail Segment Pullback 确实帮了大忙；其二是类别不平衡，class weight + sampler 组合比我们原来只用一种手段稳得多。另一个小教训是 Stage2 微调：理论上应该更好，但短程微调加随机滑窗带来的分布漂移，把收益抵消了，提醒我们“没必要的套路就别硬上”。

调参心得：学习率、warmup 比想象中敏感，尤其是带 EMA 时，稍大的 lr 会导致 EMA 权重“跟不上节奏”，验证集抖动明显；梯度裁剪和 AMP 基本是白送的稳定性提升，可以默认开。日志和 checkpoint 也要勤做，遇到 NaN 能快速回溯。

推理阶段，滑窗聚合 + 多模型加权的组合挺香，但要注意推理时长与提交截止的平衡；我们最后保留了 window TTA 的接口，但默认不开，以免超时。整体体验下来，K 折 + 加权集成是当前资源约束下最稳妥的解。

如果有更多时间，想补的方向有：更长的预训练步数（或更好的预训练数据），尝试对比 Hyena / Mamba 这类长序列模型，和更系统的阈值调优。但至少目前的方案在可靠性和复现性上已经可落地。
\end{document}