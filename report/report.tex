\documentclass[a4paper,12pt]{ctexart}
\usepackage[utf8]{inputenc}
\usepackage{geometry}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{hyperref}
\usepackage{float}
\usepackage{listings}
\usepackage{xcolor}

\geometry{left=2.5cm,right=2.5cm,top=2.5cm,bottom=2.5cm}

% 代码高亮设置
\lstset{
    basicstyle=\small\ttfamily,
    columns=flexible,
    breaklines=true,
    backgroundcolor=\color{gray!10},
    frame=single,
    keywordstyle=\color{blue},
    commentstyle=\color{green!50!black},
    stringstyle=\color{red}
}

\title{\textbf{阿里天池赛：NLP新闻分类学习赛\\实验报告}}
\author{ 郑亦航 PB23111612 \\ 李梓煊 PB23111709}
\date{\today}



\begin{document}

\maketitle

\begin{abstract}
本队伍在 NLP 新闻分类学习赛中以准确率 0.962 获得第 3 名，
成绩排名截图已置于文末附录以供证明（时间截止到 2025 年 12 月 5 日）。
\end{abstract}

\section{团队成员与分工}

本项目采用“共同决策 + 分工落地”的协作机制：在方案设计阶段由两人共同调研、讨论并决策整体技术路线，在实现阶段按职责拆分任务、各自负责落地。为在兼顾效率与质量的前提下实现分工清晰、任务解耦，我们将工作划分为前后端两个角色，具体如下：

\begin{itemize}
    \item \textbf{郑亦航}（前端：数据与模型设计）
    \begin{itemize}
        \item 负责数据探索性分析（如文本长度分布、类别不平衡等），撰写分析报告，为后续建模与训练策略提供依据。
        \item 搭建端到端的数据预处理流水线，完成数据清洗、切分、标签重映射等脚本封装，确保全流程可复现。
        \item 设计并实现 HAT 模型基础架构，定义输入管线与关键模块接口，并完成模型骨架的单元测试。
    \end{itemize}

    \item \textbf{李梓煊}（后端：训练与部署优化）
    \begin{itemize}
        \item 实现 MLM 预训练流程，完成与数据管线、Tokenizer/ID 偏移的对接与适配，保证长文本场景下的稳定训练。
        \item 优化分类训练流程，覆盖混合精度训练、超参数调整等环节，在长文本场景下提升收敛速度与性能。
        \item 负责第二阶段微调，设计与调整损失函数（如不同 Loss 组合）。
        \item 实现推理部署，基于 K-fold，将各折最佳 HAT 模型用于长文档滑窗前向与窗口聚合，在此基础上进行多模型加权集成，输出最终预测结果。
    \end{itemize}
\end{itemize}

\section{问题定义}

本次比赛的任务是“新闻文本分类”，赛题数据为新闻文本，并按照字符级别进行匿名处理，整合划分出14个候选分类类别。
\begin{itemize}
    \item \textbf{输入}：匿名处理后的字符 ID 序列（空格分隔），代表一篇新闻文本。数据经过脱敏处理，无法直接看到原始汉字。
    \item \textbf{输出}：将文本分类到 14 个类别之一（如财经、体育、娱乐等），对应的 Label ID 为 0-13。
    \item \textbf{挑战}：
    \begin{enumerate}
        \item \textbf{长文本结构复杂性}：许多样本长度远超 BERT 模型的 512 token 限制（部分达到 40,000+ token），且关键信息在文本中分布稀疏、跨段依赖长，分段式编码容易遗漏关键语义。
        \item \textbf{文本风格与领域差异}：数据来源多样，风格从新闻报道到口语评论均有，语气、表达与话题差异显著，使模型难以学习统一、稳健的特征表示。
        \item \textbf{类别极度不平衡}：在数据特征分析中发现，训练集中类别数量分布长尾明显，最大与最小类别比例高达 43:1，模型在标准训练中容易偏向头部类别。
        \item \textbf{数据噪声显著}：数据特征分析发现存在空文本、重复文本、标签冲突、低质量内容等多种噪声，会影响训练稳定性并降低模型泛化能力。
        \item \textbf{计算资源与模型容量限制}：长文本导致显存压力大，限制可使用的 batch size、对抗训练策略及模型规模，使训练不够充分。
    \end{enumerate}
\end{itemize}

\section{数据分析与预处理}

\subsection{数据特征分析}

我们对 20 万条训练样本和 5 万条测试样本进行了全面扫描，重点分析了文本长度分布和类别分布等数据特征。

\subsubsection{类别分布与不平衡}

分析结果显示，本任务为 14 类单标签分类任务，类别分布呈现极度长尾特征。
\begin{itemize}
    \item \textbf{极端不平衡}：最大类别（Label 0）拥有 38,918 条样本，而最小类别（Label 13）仅 908 条。最大最小比（Imbalance Ratio）高达 \textbf{43:1}。
    \item \textbf{策略启示}：如此悬殊的比例要求我们在 Loss 设计上引入类别平衡手段
    （如 Class Weighting 与 Focal Loss），否则模型将过度偏向样本量多的头部类别。
\end{itemize}

\subsubsection{文本长度统计}

文本长度是选择模型架构的关键依据。统计结果表明，数据包含大量超长文本：
\begin{itemize}
    \item \textbf{统计指标}：训练集文本平均长度约为 907 tokens，中位数为 676。P90 分位点达到 1,796，P99 高达 4,228，最长文本甚至接近 58,000 tokens。
    \item \textbf{覆盖率分析}：
    \begin{itemize}
        \item 若截断到 BERT 标准的 512 长度，仅能完整覆盖约 37\% 的样本。
        \item 当上下文窗口扩展至 \textbf{4096} 时，样本完整覆盖率达到 \textbf{98.89\%}，Token 级别覆盖率达到 97.37\%。超过 10k tokens 的极端长文仅占 0.08\%。
    \end{itemize}
    \item \textbf{结论}：4096 是一个兼顾信息完整度与计算开销的理想上下文窗口长度，这直接确立了我们使用长文档模型（HAT）的必要性。
\end{itemize}

\subsubsection{词汇表 (Vocab)}

数据已经过匿名化处理，Token ID 原始分布在 0--7549 之间，总词表规模约为 6,977。
\begin{itemize}
    \item \textbf{特殊 Token 冲突}：分析发现 ID 0、1、2、3、4 在原始数据中均有出现。这意味着我们不能直接使用常规的 0 (PAD)、1 (UNK) 等定义，必须进行 ID 偏移映射。
    \item \textbf{OOV 情况}：测试集中未登录词（OOV）的比例极低（仅占 token 总数的 0.0004\%），表明词表覆盖十分全面。
\end{itemize}

\subsection{数据预处理流水线}

基于上述分析，我们设计了一条轻量级的预处理流水线。值得强调，\textbf{此阶段并不进行文档分段（Segmentation）}，预处理仅负责清洗和标准化，分段逻辑交由模型输入层动态完成，以支持数据增强。

\subsubsection{Token ID 重映射}

为解决特殊 Token 冲突，我们将所有原始 Token ID 整体向右偏移 5 位：
\begin{equation}
    t' = t_{\text{raw}} + 5, \quad t_{\text{raw}} \in [0, 7549] \rightarrow t' \in [5, 7554]
\end{equation}
腾出的 [0, 4] ID 区间作如下保留，最终词表大小为 7,555：

0:\texttt{[PAD]}，1: \texttt{[UNK]}，2: \texttt{[CLS\_SEG]}，3: \texttt{[SEP]}，4: \texttt{[MASK]}

其中 2 用于段级聚合，5 用于预训练
\subsubsection{数据清洗}

我们进行了严格的数据清洗：
\begin{itemize}
    \item \textbf{去重}：移除 153 条完全重复的文本（保留出现的第一条）。
    \item \textbf{冲突处理}：移除 8 组内容相同但标签不同的冲突样本，避免错误标注干扰训练。
    \item \textbf{异常值处理}：移除长度为 0 的空文本。
\end{itemize}

\section{模型设计}

\subsection{设计目标与约束}

本赛题的核心是在有限的算力和时间内处理大规模长文本分类问题，因此，我们的方案决策基于以下关键约束：
\begin{itemize}
    \item \textbf{模型约束}：共有 20 万样本，约 2 亿 token。由于数据为匿名 ID，无法利用现有预训练模型（如 BERT/LLaMA）的语义知识，必须\textbf{从零开始预训练}模型。
    \item \textbf{数据约束}：20 万样本量较小，不能使用太大参数量的模型
    \item \textbf{长度约束}：需有效处理 4096 长度的上下文，且不能像截断 BERT 那样丢弃文本尾部的关键信息。
\end{itemize}

\subsection{架构选型对比}

基于上述约束，我们对比了四种主流技术路线，具体分析如下表所示：

\begin{table}[H]
    \centering
    \small
    \caption{长文档分类主流架构对比分析}
    \begin{tabular}{p{4cm} p{4cm} p{4cm}}
        \toprule
        \textbf{架构方案} & \textbf{优势} & \textbf{劣势} \\
        \midrule
        \textbf{大语言模型} \newline (LLaMA/Qwen 等) & 强大的生成能力与 few-shot 学习能力。 & 参数量庞大，需海量数据从零训练，容易导致过拟合；训练代价高昂。 \\
        \midrule
        \textbf{截断版 BERT} \newline (Max Len 512) & 训练速度快，参数量较少，基础完善。 & 丢失 50\% 以上的信息，性能上限受限。 \\
        \midrule
        \textbf{稀疏 Transformer} \newline (Longformer/BigBird) & 原生支持长上下文，复杂度 $O(N)$；更适合处理长文任务。 & 需自定义 CUDA Kernel 或复杂实现；在 4k 长度下显存优势不明显。 \\
        \midrule
        \textbf{层次化 Transformer} \newline (HAT/HDT) & 段内注意力提取特征，段间聚合提供全局视野；显存占用低。 & 需要定制分层结构（段内注意力结合跨段注意力）。 \\
        \bottomrule
    \end{tabular}
    \label{tab:model_comparison}
\end{table}

\textbf{决策结论}：考虑到 20 万样本足以训练一个约 1 亿参数规模的模型（相当于 BERT-Medium/Base），且 4096 长度正好可拆分为 8 个 512 长度片段，最终我们选择了 \textbf{HAT (Hierarchical Attention Transformer)} 架构。该架构在计算高效的同时，能够完整保留长文档的结构化语义信息。

\subsection{总体架构概览}

模型整体采用“分段-交互-聚合”的设计思路，自底向上包含四个核心模块：
\begin{enumerate}
    \item \textbf{动态分段器 (Document Segmenter)}：将长序列按固定长度切分为多个 Segment。
    \item \textbf{层次化嵌入层 (Hierarchical Embeddings)}：融合词、位置和段落 ID 信息以表示每个 token。
    \item \textbf{HAT Interleaved 编码器}：包含 6 层交错的 SWE+CSE 编码层，实现局部信息与全局信息的同步交互更新。
    \item \textbf{文档分类头 (Document Classifier)}：聚合段向量输出 14 维分类 logits。
\end{enumerate}

\begin{figure}[H]
\centering
\includegraphics[width=0.85\textwidth]{../report/graph/model.jpg}
\caption{模型总体架构示意图}
\label{fig:batch_memory}
\end{figure}

\subsection{动态分段器 (Dynamic Segmenter)}

分段操作在我们自定义的数据集类中动态完成。给定重映射后的序列 $x$，目标是将其切分为 $N$ 个长度为 $K=512$ 的 Segment，并确保总长度不超过 4096（即 $N \leq 8$）。

\subsubsection{尾段回拉策略 (Tail Pullback)}

直接等长切分常会导致最后一个 Segment 非常短（例如仅十几个 token），信息密度低且容易受 Padding 影响。为此，我们实现了“尾段回拉”机制：
\begin{itemize}
    \item 若切分后最后一段的剩余长度 $r < K/2$，则放弃直接按段切分，改为从文档末尾向前截取 $K$ 个 token 作为最后一段：
    \begin{equation}
        \text{Seg}_{\text{last}} = x[L-K : L]
    \end{equation}
    这样可保证每个 Segment 中都包含足够密集的语义信息。
\end{itemize}

\subsubsection{长文截断与增强}

\begin{itemize}
    \item \textbf{训练阶段 (Random Window)}：对于超过 8 个 Segment 的长文档，随机选取连续的 8 段作为训练窗口。这不仅规避了显存限制，也是一种有效的数据增强手段，提升模型对不同片段组合的鲁棒性。
    \item \textbf{推理阶段 (Sliding Window)}：采用滑动窗口策略将整篇文档分割成多个窗口（如 [0-7], [4-11] 等），分别进行前向推理，最后对各窗口的 logits 做求和或平均以得到全局预测。
\end{itemize}

\subsection{层次化嵌入层}

输入张量形状为 $[B, N, K+1]$（每个 Segment 会在起始位置自动插入 \texttt{[CLS\_SEG]} 标记）。每个 Token 的初始表示 $h_0$ 由三部分向量相加得到：
\begin{equation}
    h_0 = E_{\text{word}}[id] + E_{\text{pos\_sw}}[pos] + E_{\text{seg\_id}}[seg\_idx]
\end{equation}
其中：
\begin{itemize}
    \item \textbf{$E_{\text{word}}$}：词嵌入，维度 $[7555, 768]$。
    \item \textbf{$E_{\text{pos\_sw}}$}：段内位置嵌入，维度 $[513, 768]$，用于捕捉片段内部的位置信息。
    \item \textbf{$E_{\text{seg\_id}}$}：\textbf{段落 ID 嵌入}，维度 $[8, 768]$。这是 HAT 的关键设计之一，显式告知模型当前 Token 所属的段索引，增强模型对文档篇章结构的感知。
\end{itemize}

\subsection{HAT Interleaved 编码器}

我们采用效果最好的 Interleaved (I1) 布局，总共堆叠 6 层。每一层都包含完整的“局部-全局”交互过程：

\subsubsection{Segment-wise Encoder (SWE)}

局部语义编码：将输入张量 $H \in \mathbb{R}^{B \times N \times (K+1) \times D}$ 在 $B$ 和 $N$ 维度展平，对每个 Segment 独立执行标准 Transformer 的 Self-Attention 计算：
\begin{equation}
    H_{\text{sw}} = \text{TransformerBlock}_{\text{local}}(H)
\end{equation}

\subsubsection{Cross-segment Encoder (CSE)}

全局语境编码：从 $H_{\text{sw}}$ 中提取每个 Segment 的 \texttt{[CLS\_SEG]} 向量，加入段级位置编码 $E_{\text{pos\_cs}}$ 后，在 $N$ 个段向量之间执行 Self-Attention：
\begin{equation}
    CLS_{\text{out}} = \text{TransformerBlock}_{\text{global}}(CLS_{\text{extracted}} + E_{\text{pos\_cs}})
\end{equation}
该步骤实现了段落间的信息交互。

\subsubsection{全局信息回注 (Information Injection)}

这是 Interleaved 结构的核心步骤。将 CSE 更新后的全局段向量 $CLS_{\text{out}}$ 通过投影矩阵 $W_g$ 映射后，\textbf{广播加回}对应段内的所有 Token 表示：
\begin{equation}
    H_{\text{next}}[i, j] = H_{\text{sw}}[i, j] + W_g \cdot CLS_{\text{out}}[i]
\end{equation}
这使得每一层中底层的 Token 表示都能实时融合文档级全局语境信息，避免传统层次模型“先局部后全局”处理导致的信息割裂。

\subsection{文档分类头}

经过 6 层编码后，提取所有 Segment 的 \texttt{[CLS\_SEG]} 向量，通过 Masked Mean Pooling 得到整个文档的表示 $h_{\text{doc}}$，然后经过 LayerNorm 和 Dropout，最后接入全连接层输出 14 维 logits，代表对每个类别的预测分数。

\subsection{模型配置总结}

\begin{table}[h]
    \centering
    \small
    \caption{HAT-Interleaved-512 模型详细配置}
    \label{tab:hat-config}
    \begin{tabular}{ll|ll}
        \toprule
        \textbf{参数} & \textbf{数值} & \textbf{参数} & \textbf{数值} \\
        \midrule
        Vocab Size & 7,555 & Layers (SWE + CSE) & 6 \\
        Hidden Size & 768 & Segment Length & 512 \\
        Heads & 12 & Max Segments & 8 \\
        FFN Size & 3072 & Max Context & 4096 \\
        \bottomrule
    \end{tabular}
\end{table}
模型配置如表~\ref{tab:hat-config} 所示，模型参数大小约为 95M。

\section{训练与调优}

\subsection{预训练（MLM）}

在正式进行分类任务前，我们先利用 Masked Language Modeling（MLM）在新闻语料上进行预训练，让模型预先学习领域语义分布。

\paragraph{设置与流程}

\begin{itemize}
    \item 我们使用预处理后的训练数据集，随机打乱后按 9:1 划分为训练集和验证集。
    \item 每篇长文档切分为最多$N$个片段，每段长度固定为 $K=512$，尽可能覆盖全文。
    \item 我们以 15\% 的概率将部分 token 进行 Mask，并对输入和标签进行 padding，以对齐批处理的形状。
\end{itemize}

\paragraph{技术手段}

预训练阶段，我们基于 HAT-MLM 架构，结合 AdamW 优化器、线性 warmup 与学习率衰减、EMA 等技术，构建了一条稳定可复现的预训练流程，主要配置如下：
\begin{itemize}
    \item 模型采用 HAT-MLM 架构，默认 batch size 为 4，总训练步数约 10000 步。数据读入并打乱后按 9:1 划分训练集和验证集。
    \item 优化器为 AdamW，初始学习率为 5e-5，权重衰减 0.01，$\texttt{betas}=(0.9, 0.999)$，$\texttt{eps}=1\text{e-}8$，并且在反向传播中使用梯度裁剪（阈值 1.0）。
    \item 学习率策略：前 6\% 训练步数线性 warmup，随后线性衰减至 0。
    \item 为提高训练稳定性，启用 EMA（指数滑动平均，衰减系数 0.999）。
\end{itemize}

\paragraph{训练效果}

\begin{table}[h]
    \centering
    \small
    \caption{MLM 预训练验证集指标}\label{tab:mlm_metrics}
    \begin{tabular}{cccc}
        \hline
        \textbf{Step} & \textbf{验证 Loss} & \textbf{验证 PPL} & \textbf{备注} \\
        \hline
        200   & 8.53  & 5064.05 & 起始值对照 \\
        1000  & 6.50  & 665.12  & 骤降 \\
        3000  & 5.60  & 270.06  & 持续收敛 \\
        5000  & 4.08  & 59.42   & 首次低于 60 \\
        7000  & 2.89  & 17.97   & PPL 快速降低 \\
        9000  & 2.57  & 13.10   & 进入稳定期 \\
        11000 & 2.43  & 11.41   & 收敛尾声 \\
        12000 & \textbf{2.41} & \textbf{11.08} & 最终/最佳 \\
        \hline
    \end{tabular}
\end{table}

从表~\ref{tab:mlm_metrics} 可以看出，验证集困惑度（PPL）从 Step 200 的 5064 快速下降到 Step 12000 的 11.08，且全程未出现反弹。预训练所得的最佳模型参数随后用于分类任务的微调初始化。

\subsection{分类模型训练（Stage1）}

\paragraph{整体流程}

分类阶段我们采用分层 K 折交叉验证，默认使用分层 5 折（Stratified K-Fold，随机种子 42）。在保证各折标签分布大致均衡的前提下，依次训练并保存每一折的最佳模型，供后续推理集成使用。

\paragraph{数据与模型}

使用预处理后的新闻文本及其标签作为训练语料。对于每篇长文档，按固定长度将其切分成若干片段，
并在批处理阶段打包成形状为 \([B,N,K]\) 的张量输入模型。
分类模型基于 HAT 架构实现，内部会自动添加 \texttt{[CLS\_SEG]} 标记并输出文档级 logits
\paragraph{损失函数与优化策略}

训练过程中综合使用了多种分类损失，包括交叉熵、标签平滑以及 Focal Loss 。默认标签平滑系数为 0.05，Focal Loss 的 $\gamma=2.0$，并结合预先计算的类别权重以及可选的样本加权采样，以缓解类别不平衡。batch size 默认为 64（训练）和 128（验证），反向传播阶段采用梯度裁剪（阈值 1.0）。

优化器选用 AdamW（初始学习率 3e-5，权重衰减 0.01，$\texttt{betas}=(0.9, 0.999)$，$\texttt{eps}=1\text{e-}8$）。前 6\% 总步数进行学习率 warmup，然后线性衰减至 0。默认训练 5 个 epoch，并结合 early stopping 策略，当验证指标长时间无提升时提前结束训练。

为兼顾效率与稳定性，我们启用了 AMP 混合精度训练以及 EMA（衰减系数 0.9999）移动平均，验证时使用 EMA 权重进行评估。训练日志按 step 记录，每折使用独立随机种子，保证结果可稳定复现，便于后续集成各折最佳 checkpoint。

\paragraph{验证与模型选择}

每个 epoch 结束后，我们在当前折的验证集上评估 Loss、准确率和 Macro-F1，选取 Macro-F1 最高的模型作为该折的最佳模型；同时采用 early stopping，如果验证指标若干轮未提升则提前终止训练，防止过拟合并节约计算资源。

\begin{table}[h]
    \centering
    \small
    \caption{Stage1 各折验证集最佳分数}\label{tab:stage1_metrics}
    \begin{tabular}{ccc}
        \hline
        \textbf{Fold} & \textbf{最佳验证 Macro-F1} & \textbf{Checkpoint 文件名} \\
        \hline
        1 & 0.9597 & \texttt{hat\_cls\_fold0\_best.pt} \\
        2 & 0.9569 & \texttt{hat\_cls\_fold1\_best.pt} \\
        3 & 0.9609 & \texttt{hat\_cls\_fold2\_best.pt} \\
        4 & 0.9612 & \texttt{hat\_cls\_fold3\_best.pt} \\
        5 & 0.9625 & \texttt{hat\_cls\_fold4\_best.pt} \\
        \hline
        \textbf{平均} & \textbf{0.9602 ± 0.0019} &  \\
        \hline
    \end{tabular}
\end{table}

从表~\ref{tab:stage1_metrics} 可以看出，5 折训练耗时共约 946 分钟，各折最佳模型均来自 EMA 权重。后续推理阶段我们将集成以上 5 个最佳模型。

\subsection{第二阶段微调（Stage2）}

\paragraph{动机与设定}

在第一阶段训练基础上，我们尝试对每折的最佳模型进行小学习率的二次微调，希望通过更强的正则化和轻量数据增强进一步提升性能。

\paragraph{训练设置}

\begin{itemize}
    \item 继续使用相同的 5 折数据划分，以各折 Stage1 最佳模型 (\texttt{hat\_cls\_fold\{k\}\_best.pt}) 作为初始权重。
    \item 采用更小的学习率进行 AdamW 优化（lr = 1e-5，weight decay 0.01），训练 1$\sim$2 个 epoch，保留梯度裁剪和 AMP 混合精度。
    \item 仅在训练阶段随机调整滑窗起点（token 级偏移），增加长文档训练的“视角多样性”，并可选用 R-Drop、FGM 等正则化手段提升泛化能力。
\end{itemize}

\paragraph{结果与分析}

在实际实验中，如表~\ref{tab:stage2_metrics} 所示，Stage2 的 Macro-F1 相较 Stage1 整体提升不明显，部分折甚至略有下降。最终提交结果中我们仍采用 Stage1 权重进行推理。初步分析原因包括：

\begin{table}[h]
    \centering
    \small
    \caption{Stage2 微调验证结果（部分折）}\label{tab:stage2_metrics}
    \begin{tabular}{cccc}
        \hline
        \textbf{Fold} & \textbf{Stage2 最佳 F1} & \textbf{Stage1 最佳 F1} & \textbf{Stage2 是否提升} \\
        \hline
        1 & 0.9572 & 0.9597 & 否 \\
        2 & 0.9541 & 0.9569 & 否 \\
        3 & 0.9565 & 0.9609 & 否 \\
        \hline
    \end{tabular}
\end{table}

\begin{itemize}
    \item 第一阶段训练已将模型性能推至较高水准，小步长、少迭代的二次微调改进空间有限。
    \item 额外的正则化手段（如 R-Drop、对抗训练）在短暂微调阶段带来的扰动大于收益。
    \item 随机滑窗导致的数据分布轻微漂移，在有限训练步数内模型难以完全重新收敛，反而可能影响原有决策边界。
\end{itemize}

\section{推理与部署}

\subsection{整体流程}

推理阶段通过统一调度脚本进行控制：它会收集各折模型权重（若存在 Stage2 则优先使用 Stage2 权重，否则使用 Stage1 模型），对长文档执行滑窗推理、窗口聚合，并将多个模型结果加权集成，最终生成提交文件。

\subsection{滑窗与特征构建}

\begin{itemize}
    \item 在推理数据集中为每条样本保留原始文档 ID，并动态生成对应的滑窗序列。
    \item 将每个文档的滑窗序列 padding 到统一长度（如 \([N,K]\) = 8×512），再堆叠成批次输入模型。
    \item 支持通过不同的滑窗偏移为同一文档生成多组窗口，以提升模型鲁棒性。
\end{itemize}

\subsection{集成与决策}

\begin{itemize}
    \item 首先，对同一文档的多个滑窗预测结果（logits）进行聚合（如取均值、最大值或按置信度加权平均），得到文档级别的 logits。
    \item 然后，在模型维度对各模型的预测进行融合，可选择对 logits 求平均或对 softmax 概率求平均，并支持根据验证集 macro-F1 对模型进行加权。
    \item 还支持可选的阈值策略：二分类任务可设置统一阈值，多分类任务可为不同类别单独设置阈值；若有验证集，可通过网格搜索自动调整阈值。
\end{itemize}

\subsection{分布式推理部署}

使用多 GPU 推理时，脚本自动用 \texttt{torchrun} 启动多进程并 完成通信初始化，分布式采样将滑窗数据按进程划分以避免重复且保持顺序；各进程先独立完成窗口聚合与模型集成，
随后用汇总至 rank0 生成提交文件。

而当未指定多进程参数则回退单进程，并可按节点 GPU 数自动设置进程数、通信地址与显存策略以保证稳定性。

\section{实验总结与感悟}

这次进行长文本分类任务，我们最大的体会是：先搭建起基本流程，再逐步优化细节。一开始我们幻想能一下子用上所有复杂技巧，但受限于显存和时间，这种做法并不可行。转而采用最小可行方案完成初步流程，再逐步替换为 HAT、滑窗、集成等组件，反而使节奏更加平稳。

长文本处理过程中有两个难点令人印象深刻：其一是分段策略。若末尾片段过短，关键信息可能几乎完全丢失，Tail Segment Pullback 策略在这方面确实发挥了关键作用；其二是类别不平衡问题。采用类别权重结合采样的方法比我们原先只使用单一手段效果稳定得多。另一个教训是第二阶段微调：理论上应该有所提升，但短暂微调加上随机滑窗造成的数据分布变化抵消了收益。这也提醒我们：没有必要的技巧不必强行使用。

关于参数调优的心得：学习率和 warmup 设置比预想中更为敏感，尤其在启用 EMA 时，学习率稍大就可能导致 EMA 权重无法及时跟上模型更新速度，使验证集指标出现明显波动；梯度裁剪和 AMP 则基本都能提供稳定性提升，应默认开启。还应勤于记录日志并保存检查点，以便训练出现 NaN 问题时能快速定位问题。

推理阶段，滑窗聚合结合多模型加权的方案效果显著，但需要注意推理耗时与提交截止时间之间的平衡。我们在最终方案中保留了滑窗 TTA 接口，但默认关闭以避免超时。在当前资源约束下，K 折训练加权集成是最稳妥的解决方案。

如果有更多时间，我们希望完善的方向包括：延长预训练步数（或采用更高质量的预训练数据）、尝试引入 Hyena、Mamba 等新型长序列模型，以及对阈值进行更系统的调优。不过当前的方案在可靠性和复现性上已经达到较为满意的程度。

\clearpage
\appendix
\section{成绩排名截图}
\begin{figure}[!h]
    \centering
    \includegraphics[width=0.8\textwidth]{graph/rank.png}
    \caption{成绩排名截图}
\end{figure}
\end{document}
